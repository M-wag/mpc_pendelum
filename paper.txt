Abstract:
    We chose explore model predictive control for the pendelum problem. Neural ODE's are used for the dynamics model. 

Intro:
    Motivation of extension
	In a recent talk Lecun, outlined his predictions for fturue AI
	In it he explains how more intelligent AI will consit of more specialized modules
	One such model is the World Model which the agent can use for logn-term planning 
	In RL terms, what he refers to is MPC

	From the perspective of Neuroscience, specifically those aiming to utilize Artificial Neural Networks(ANNs)(Neuroconnectionist) Reinforcement Learning is especially interesting. The reason being that it is much more realistic for biological organisms to learn like this relative to supervised learning. An algorithm like model predictive control seems great for modeling planning in the context of cognitive science. 
	
	A second motivataion for this extension is further exploration of Neural ODEs. 


    Research Quesiton and Hypothesis
	We aim to study under what circumstancesMPC will be beneficial for the pendelum problem. Specifically, is there a ceiling effect? Is MPC control better when the policy is not as effective.
	

Methods 
    Real Environment
	The task we want our agent to solve in the pendelum problem. We use the dedicated gymnax "Pendelum-v1" environment, with default parameters(Table 1)

	max_speed: float = 8.0
	max_torque: float = 2.0
	dt: float = 0.05
	g: float = 10.0  # gravity
	m: float = 1.0  # mass
	l: float = 1.0  # length
	max_steps_in_episode: int = 200

	The state space consits of a specific angle on a radius circle along with the associated angular velocity.

	The starting position is randomly initialized using the environment defaults. Which initializes the theta between pi and negative pi and an angular velocity between -1 and 1. 

    RL Agent and Policy
	For the policy we use a fully connected multi layer perceptron with a single hidden layer with 65 neurons and ReLU aciivations and a softmax . The policy takes as input the theta in polar coordinates, cosine and sine respectively and the third input is the theta dot. We apply a softmax to obtain the probality distibution for the actions.

	We restrict the action space to 3 options. Moving left by a set amount, moving right by a set amount or doing nothing.

    Reward Function	
	At each timestep during the rollout there is an associated reward. We utilize the default from the gymnax environment calculated using:

	angle_normalize(theta) ** 2
	+ 0.1 * state.theta_dot ** 2
	+ 0.001 * (u ** 2)

	The actual metric we end up using is the return, which is the cumulative reward the agent expects during each timestep of the rollout. Rewards in the future are usually discounted by some factor so are less influential for the return at that given timestep.

	We apply baseline correction, i.e. removing the mean from all the data. We noticed that some initilizations caused the agent to fall into a local minima and would heavily decrease the rate at which the model converge. After applying baseline correction we notice that all tested initilizations steadily converege to -200 re

    Learning Rules Policy
	We use the REINFORCE algorithm to update the policy gradients. The current policy of the agent is used to generate samples during rollout. The direction of the gradient is determined by the return. 
	[Insert Function] 
	[Insert Hyperparameters]
	[Insert Optimizers]

    Neural ODEs for Dynamics
	The NeuralODE is essentially just a differential equation solver where the function being solved is a multi layer peceptron. The MLP is has a depth of 3 with a width of 22 and uses softplus activation functions. Training the model essentially solves giving it the inital state along with a control signal and seeing how well it learns. You specify the time over which you want the model to solve. Rollout is done by solving for only one timestep, seeing how the agent acts and then using that input for the next step. Importantly the real environment both has a state, and a observation. The ODE exclusively models the observations.

    Learning Rules Dynamics Model
	Gradients are updated by taking the Mean-Squared Error loss between the outcomes from the real system and the Neural ODE's predicted outcomes. To avoid getting stuck in a local minima we employ only train the model on a first portion of the data until the model converges. We then increase the fraction the model is trained on and append it to the dataset. We do this until the model has been trained on the entire length of the sequences.

	[Hyper parametrs]
	[Optimzier]

    Training Policy and ODE in unison
	Our basic loop consists of initializing a random policy. Generating data by rolling out this policy on the real system. Using this data to train the neural ODE. And then training the policy on the neural ODE so it gets access to more of the obseration space.

    Evaluation Metrics

Results 
    Although a pipeline was made to train the policy on the trained ODE, training ended up not succesful due to a bug in the code. Somewhere a NaN value shows up meaning the weights of both the dynamics model and the policy seem to get corrupted. Preventing them from being able to learn. It was not possible to find the cause of this in the limited time. We assume this stems from our code in rollouing out the agent on the ODE.

    However we did evaluate the training of the agent on the real environment. Which performed identically to the assignment, implying the code was succesfully reimplemented. Training the Neural ODE on the real environment also manged to reporoduce good results

    [Loss of agent ]
    [Rolllout agent]
    
    [Loss of NODE]
    [ Dynamics of NODE]

    To evaluate the role of MPC on the pendelum task anyway we evaluate MPC using trained ODE and policy which did not interact with each other

Conclusion
    
Discussion
    Although we were not succesful in implementing MPC using our initial plan. We do see that it is possible to learn the dynamics of the pendelum using a very simple Neural ODE. 




Future work
    Hypothesis about hardware and neuroscience algorithms
    Testing hypothesis on neuromorphic hardware
    Given some inspired architecture what algorithm is most appropriate
    Analytical methods to get the optimal rate of recurrence and parallism 
